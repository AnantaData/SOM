{
 "metadata": {
  "name": "",
  "signature": "sha256:30e0d645a15a1c2feb862aeb0af0eeed6f3426ffad9bd2c10088cfd99b6b6e95"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Import Dependencies"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Import the necessary dependencies\n",
      "import random\n",
      "import numpy as np\n",
      "import math\n",
      "import sys"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Define Class k_map for the self organizing map"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "The functions will be described in the code below"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Also note that here, we have used a three-dimensional array instead of an object-oriented approach for implementing the data structures for he SOM."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "class k_map(object):\n",
      "    \n",
      "    iters=0\n",
      "    num_x=0\n",
      "    num_y=0\n",
      "    inps=0\n",
      "    neurons=[]\n",
      "    temp=None\n",
      "    radius=0\n",
      "    \n",
      "    '''\n",
      "    Constructor Function: Takes as input the following parameters\n",
      "    \n",
      "    x_size : the width of the map desired\n",
      "\n",
      "    y_size : the height of the  map desired\n",
      "    \n",
      "    input_dims : the dimensinality of the vectors the som is trained to deal with\n",
      "    \n",
      "    decay: A constant decay rate, which is actually a learning rate. The coefficients of the parts where I  use the \n",
      "    'delta rule' will continually be multiplied by the decay to reduce it. Hence the 0 < decay < 1 (exclusive)\n",
      "    \n",
      "    alpha_start: Starting constant of the coefficient of delta. This should also be 1 >= alpha > 0. This will gradually\n",
      "    decline owing to the decay rate\n",
      "    \n",
      "    alpha_min: the termination of batch learning will be determined when the alpha is lower than this bound. Hence this\n",
      "    is the lower bound for this.\n",
      "    \n",
      "    '''\n",
      "    \n",
      "    def __init__(self,x_size,y_size,input_dims,decay,alpha_start,alpha_min):\n",
      "        self.d_rate=decay\n",
      "        self.al_min=alpha_min\n",
      "        self.alpha=alpha_start\n",
      "        self.num_x=x_size\n",
      "        self.num_y=y_size\n",
      "        self.inps=input_dims\n",
      "        self.neurons=np.random.randint(100,size=(x_size,y_size,input_dims))\n",
      "        self.dissim=np.ndarray(shape=(self.num_x,self.num_y))\n",
      "        self.radius=min(x_size,y_size)/2\n",
      "        \n",
      "    def print_nw(self):\n",
      "        for i in range(0,self.num_x):\n",
      "            for j in range(0,self.num_y):\n",
      "                print 'number',i,j\n",
      "                print self.neurons[i][j]\n",
      "        return\n",
      "    \n",
      "    '''\n",
      "    The adjust method will adjust the weight of the given neuron (node in the map) to be closer to a target vector\n",
      "    A simple application of the delta rule is applied  here\n",
      "    '''\n",
      "    \n",
      "    def adjust(self,neuron,target,alpha):\n",
      "        for i in range(neuron.shape[0]):\n",
      "            neuron[i]= neuron[i]+(target[i]-neuron[i])*alpha\n",
      "            \n",
      "        return    \n",
      "    \n",
      "    '''\n",
      "    The reweight method iteratively applies the adjust subroutine to adjust the weights of an area that falls under the\n",
      "    radius (which also declines with time)\n",
      "    \n",
      "    '''\n",
      "    \n",
      "    def reweight(self,target,center,nmap,alpha):\n",
      "        radius = int(self.radius*alpha)\n",
      "        for i in range(nmap.shape[0]):\n",
      "            for j in range(nmap.shape[0]):\n",
      "                if np.linalg.norm(np.array([i,j])-np.array(center)) > radius:\n",
      "                    continue\n",
      "                self.adjust(nmap[i][j],target,alpha)            \n",
      "        return\n",
      "    \n",
      "\n",
      "    \n",
      "################################################################################################\n",
      "    '''\n",
      "    calc_dissims calculate the dissimilarity measures (here the eculidean distance) to all the neurons for a given\n",
      "    training/input vector and returns the closestly similar vector's coordinates as output\n",
      "    '''\n",
      "\n",
      "    def calc_dissims(self,input_vector):\n",
      "        self.dissim=np.zeros(shape=(self.num_x,self.num_y))\n",
      "        for x in range(self.neurons.shape[0]):\n",
      "            for y in range(self.neurons.shape[1]):\n",
      "                self.dissim[x][y] = np.linalg.norm(self.neurons[x][y]-input_vector)\n",
      "        max_fit=np.array([np.argmin(self.dissim)%self.num_x, np.argmin(self.dissim)/self.num_x])        \n",
      "        return max_fit\n",
      "    \n",
      "################################################################################################\n",
      "    '''\n",
      "    Train a map with a single input. \n",
      "    '''\n",
      "    def train_map(self,input_vector):\n",
      "        \n",
      "        self.reweight(input_vector,self.calc_dissims(input_vector),self.neurons,self.alpha)\n",
      "        return\n",
      "    '''\n",
      "    Applies the train_map method iteratively to a batch of training vectors. Encouraged to use this rather than the\n",
      "    single training method\n",
      "    '''\n",
      "    def batch_train(self,input_batch):\n",
      "        print 'alpha = ',self.alpha,'alpha_min=',self.al_min\n",
      "        while self.alpha > self.al_min:\n",
      "            for inp in input_batch:\n",
      "                self.train_map(inp)\n",
      "            self.alpha = self.d_rate*self.alpha\n",
      "            self.iters+=1\n",
      "        print self.alpha\n",
      "        print 'done in ',self.iters, 'iterations...'\n",
      "        return\n",
      "    \n",
      "    '''\n",
      "    gives as output the coordinates of the neuron of which the weights bear the closest resemblance to the input\n",
      "    vector. \n",
      "    '''\n",
      "    def predict(self,input_vector):\n",
      "        \n",
      "        output=self.calc_dissims(input_vector)\n",
      "        \n",
      "        return output\n",
      "        \n",
      "    '''\n",
      "    The above, applied in a batch manner. Please try to figure out what is wrong with this thing...\n",
      "    '''\n",
      "    def cluster(self,input_batch):\n",
      "        \n",
      "        target=np.ndarray(shape=(input_batch.shape[0]))\n",
      "        i=0;\n",
      "        for inp in input_batch:\n",
      "            target[i]=np.array(self.predict(inp))\n",
      "        print 'done...'\n",
      "        return target\n",
      "        \n",
      " "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%save kmap.py 6 7"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "name": "stdout",
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "File `kmap.py` exists. Overwrite (y/[N])?  y\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The following commands were written to file `kmap.py`:\n",
        "#Import the necessary dependencies\n",
        "import random\n",
        "import numpy as np\n",
        "import math\n",
        "import sys\n",
        "\n",
        "class k_map(object):\n",
        "    \n",
        "    iters=0\n",
        "    num_x=0\n",
        "    num_y=0\n",
        "    inps=0\n",
        "    neurons=[]\n",
        "    temp=None\n",
        "    radius=0\n",
        "    \n",
        "    '''\n",
        "    Constructor Function: Takes as input the following parameters\n",
        "    \n",
        "    x_size : the width of the map desired\n",
        "\n",
        "    y_size : the height of the  map desired\n",
        "    \n",
        "    input_dims : the dimensinality of the vectors the som is trained to deal with\n",
        "    \n",
        "    decay: A constant decay rate, which is actually a learning rate. The coefficients of the parts where I  use the \n",
        "    'delta rule' will continually be multiplied by the decay to reduce it. Hence the 0 < decay < 1 (exclusive)\n",
        "    \n",
        "    alpha_start: Starting constant of the coefficient of delta. This should also be 1 >= alpha > 0. This will gradually\n",
        "    decline owing to the decay rate\n",
        "    \n",
        "    alpha_min: the termination of batch learning will be determined when the alpha is lower than this bound. Hence this\n",
        "    is the lower bound for this.\n",
        "    \n",
        "    '''\n",
        "    \n",
        "    def __init__(self,x_size,y_size,input_dims,decay,alpha_start,alpha_min):\n",
        "        self.d_rate=decay\n",
        "        self.al_min=alpha_min\n",
        "        self.alpha=alpha_start\n",
        "        self.num_x=x_size\n",
        "        self.num_y=y_size\n",
        "        self.inps=input_dims\n",
        "        self.neurons=np.random.randint(100,size=(x_size,y_size,input_dims))\n",
        "        self.dissim=np.ndarray(shape=(self.num_x,self.num_y))\n",
        "        self.radius=min(x_size,y_size)/2\n",
        "        \n",
        "    def print_nw(self):\n",
        "        for i in range(0,self.num_x):\n",
        "            for j in range(0,self.num_y):\n",
        "                print 'number',i,j\n",
        "                print self.neurons[i][j]\n",
        "        return\n",
        "    \n",
        "    '''\n",
        "    The adjust method will adjust the weight of the given neuron (node in the map) to be closer to a target vector\n",
        "    A simple application of the delta rule is applied  here\n",
        "    '''\n",
        "    \n",
        "    def adjust(self,neuron,target,alpha):\n",
        "        for i in range(neuron.shape[0]):\n",
        "            neuron[i]= neuron[i]+(target[i]-neuron[i])*alpha\n",
        "            \n",
        "        return    \n",
        "    \n",
        "    '''\n",
        "    The reweight method iteratively applies the adjust subroutine to adjust the weights of an area that falls under the\n",
        "    radius (which also declines with time)\n",
        "    \n",
        "    '''\n",
        "    \n",
        "    def reweight(self,target,center,nmap,alpha):\n",
        "        radius = int(self.radius*alpha)\n",
        "        for i in range(nmap.shape[0]):\n",
        "            for j in range(nmap.shape[0]):\n",
        "                if np.linalg.norm(np.array([i,j])-np.array(center)) > radius:\n",
        "                    continue\n",
        "                self.adjust(nmap[i][j],target,alpha)            \n",
        "        return\n",
        "    \n",
        "\n",
        "    \n",
        "################################################################################################\n",
        "    '''\n",
        "    calc_dissims calculate the dissimilarity measures (here the eculidean distance) to all the neurons for a given\n",
        "    training/input vector and returns the closestly similar vector's coordinates as output\n",
        "    '''\n",
        "\n",
        "    def calc_dissims(self,input_vector):\n",
        "        self.dissim=np.zeros(shape=(self.num_x,self.num_y))\n",
        "        for x in range(self.neurons.shape[0]):\n",
        "            for y in range(self.neurons.shape[1]):\n",
        "                self.dissim[x][y] = np.linalg.norm(self.neurons[x][y]-input_vector)\n",
        "        max_fit=np.array([np.argmin(self.dissim)%self.num_x, np.argmin(self.dissim)/self.num_x])        \n",
        "        return max_fit\n",
        "    \n",
        "################################################################################################\n",
        "    '''\n",
        "    Train a map with a single input. \n",
        "    '''\n",
        "    def train_map(self,input_vector):\n",
        "        \n",
        "        self.reweight(input_vector,self.calc_dissims(input_vector),self.neurons,self.alpha)\n",
        "        return\n",
        "    '''\n",
        "    Applies the train_map method iteratively to a batch of training vectors. Encouraged to use this rather than the\n",
        "    single training method\n",
        "    '''\n",
        "    def batch_train(self,input_batch):\n",
        "        print 'alpha = ',self.alpha,'alpha_min=',self.al_min\n",
        "        while self.alpha > self.al_min:\n",
        "            for inp in input_batch:\n",
        "                self.train_map(inp)\n",
        "            self.alpha = self.d_rate*self.alpha\n",
        "            self.iters+=1\n",
        "        print self.alpha\n",
        "        print 'done in ',self.iters, 'iterations...'\n",
        "        return\n",
        "    \n",
        "    '''\n",
        "    gives as output the coordinates of the neuron of which the weights bear the closest resemblance to the input\n",
        "    vector. \n",
        "    '''\n",
        "    def predict(self,input_vector):\n",
        "        \n",
        "        output=self.calc_dissims(input_vector)\n",
        "        \n",
        "        return output\n",
        "        \n",
        "    '''\n",
        "    The above, applied in a batch manner. Please try to figure out what is wrong with this thing...\n",
        "    '''\n",
        "    def cluster(self,input_batch):\n",
        "        \n",
        "        target=np.ndarray(shape=(input_batch.shape[0]))\n",
        "        i=0;\n",
        "        for inp in input_batch:\n",
        "            target[i]=np.array(self.predict(inp))\n",
        "        print 'done...'\n",
        "        return target\n",
        "        \n",
        " \n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Sample creating of a map"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "somap = k_map(5,5,3,0.99,0.9,0.1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 70
    }
   ],
   "metadata": {}
  }
 ]
}